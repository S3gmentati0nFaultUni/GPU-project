\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{src/preamble}
\usepackage[
	backend=biber,
	maxalphanames=10,
]{biblatex}
\bibliography{bibliography.bib}

\begin{document}

\noindent
\begin{center}
	\textbf{{EFFICIENT SOLUTION FOR MST COMPUTATION ON SPARSE GRAPHS}} \\
\end{center}

\noindent
\textbf{Author: Alessandro Biagiotti} \hfill \textit{Milan university}
\\

\noindent
\textbf{ABSTRACT:}
\\
%TODO Add a link to the github page
In this report I will be going over the process that lead to the development of the solution for \mst present at GITHUB. The solution was implemented following~\cite{generic-he-boruvka} and the results presented in the original paper will be used extensively in the following as comparison metrics.
\\

\noindent
\textbf{KEYWORDS:}
\\

\bigskip

\phantomsection
\makeatletter\def\@currentlabel{\texttt{(I)}}\makeatother\label{sec:intro}
\noindent
\textbf{INTRODUCTION TO THE PROBLEM:}
\\
The Minimum Spanning Tree Problem (\mstp for short) is a problem that has been studied for years and to this day finds many real world applications,to name a few:
\begin{enumerate}
	\item It's possible to find different \mst-based techniques that can be used to do image segmentation~\cite{maze-generation}~\cite{mst-segmentation-heuristic}.
	\item Finding the \mst allows us to solve the Clustering problem, all we need to do is to compute an \mst and then drop the $k - 1$ most expensive edges of the \mst~\cite{mst-applications}.
	\item Finding the \mst is an important step of Christofides's algorithm, which is a 2-approximation for the widely known Travelling Salesman Problem (\textsc{Tsp} for short)~\cite{tsp-christofides}.
\end{enumerate}

Many different solutions have been found for the problem in the years, the more basic implementations were Prim's~\cite{prim-algorithm} and Kruskal's~\cite{kruskal-algorithm}.

Prim's algorithm is the simplest solver for \mstp, given a graph $G= (V, E)$, let us suppose that the size of the graph $|V|$ is $n$ and the number of edges $|E|$ is $m$, then, based on the implementation, the computational complexity ranges between the following values:
\begin{itemize}
	% TODO Fix the ï not showing properly
	\item Naïve implementation: $\O(n^2)$
	\item Binary heap and adjacency list: $\O(m\log{n})$
	\item Fibonacci heap and adjacency list: $\O(m + n\log{n})$
\end{itemize}
In my implementation I have implemented both the Naïve version and the one using a binary heap, more on this in the next section.

Kruskal's algorithm is the alternative to Prim's algorithm and it works in time $\O(m\log{m})$ if implemented using classical data structures.

\brka's algorithm is a solver for the \mstp based on forests, a more verbose version of the algorithm shown in~\cite{boruvka-pseudocode} can be seen in~\ref{algo:boruvka-pseudocode}
\begin{algorithm}
	\caption{\brka's algorithm}
	\label{algo:boruvka-pseudocode}
	\begin{algorithmic}[1]
		\REQUIRE An undirected, connected and weighted graph $G$, a set of empty edges $T$
		\ENSURE An \mst$T$ built on graph $G$
		\WHILE{vertices in $G$ connected by $T$ are disjoint}
			\STATE start with an empty set of edges $E$
			\FOR{every connected component}
				\STATE start with an empty set of edges $S$
				\FOR{every vertex $v$ in the component}
					\STATE add the cheapest edge going from $v$ to any other component to $S$
				\ENDFOR
				\STATE add the cheapest edge in $S$ to $E$
			\ENDFOR
			\STATE add the set of edges from $E$ to $T$ 
		\ENDWHILE
		\STATE\RETURN $T$
	\end{algorithmic}
\end{algorithm}

Let us suppose that the size of the graph $|V|$ is $n$ and the number of edges $E$ is $m$, then the computational complexity of the pseudocode shown in~\ref{algo:boruvka-pseudocode} is $\O(m\log{n})$. This algorithm has been later reused in combination with other techniques to compute a solution for the \mstp very efficiently~\cite{boruvka-ackermann}~\cite{karger-klein-tarjan}. As for the solutions previously proposed for the sequential resolution of the problem there is different parallel implementations that are based on \brka's solution, this is due to the inherent parallelizability of the algorithm.

Regardless of the implementation the core of the algorithm can be identified in the following four steps as identified in~\cite{boruvka-steps}:
\begin{enumerate}
	\item\label{item:first-step} (\textit{choose lightest}) for every vertex the lightest edge is chosen in parallel.
	\item\label{item:second-step} (\textit{find root}) for every vertex we find the root of the tree to which it belongs\footnote{It's important to keep in mind that vertices are actually supervertices.}.
	\item\label{item:third-step} (\textit{rename vertices}) Since the graph after~\ref{item:fourth-step} is compressed, which means that the number of vertices decreases, then the roots in the graph need to be renamed. 
	\item\label{item:fourth-step} (\textit{graph compression}) The graph undergoes a compression step, the result a compressed version of the graph containing only the roots identified in~\ref{item:second-step} and the edges that connect a component $C_i$ to any other component $C_j$, therefore the graph only loses the edges contained in the original graph.
\end{enumerate}

The specific implementation that I chose to follow is shown in~\cite{generic-he-boruvka} and is summarized by algorithm~\ref{algo:boruvka-parallel}

\begin{algorithm}
	\caption{\brka's algorithm}\label{algo:boruvka-parallel}
	\begin{algorithmic}[1]
		\REQUIRE An undirected, connected and weighted graph $G(V, E)$
		\ENSURE An \mst$T$ built on graph $G$
		\WHILE{$|V| > 1$}
			\STATE find the minimum edge per vertex (\ref{item:first-step})
			\STATE remove mirrored edges
			\STATE initialize the colors
			\WHILE{the coloring hasn't converged}
				\STATE color propagation (\ref{item:second-step})
			\ENDWHILE
			\STATE create new vertex ids (\ref{item:third-step})
			\STATE do graph compression (\ref{item:fourth-step})
		\ENDWHILE
		\STATE\RETURN $G$
	\end{algorithmic}
\end{algorithm}

The graph returned in the last step is a single supervertex containing the \mst.

\bigskip
\phantomsection
\makeatletter\def\@currentlabel{\texttt{(II)}}\makeatother\label{sec:graph-structure}
\noindent
\textbf{GRAPH STRUCTURE:}
\\
Before moving to discussing the solution I want to introduce the way I implemented the graph structure. When writing code running on the CPU it's common to implement a graph using one of two alternatives:
\begin{itemize}
	\item The adjacency matrix, which represents any connection between two vertices, $i, j$ in the graph as a $1$ or $0$ in the matrix\footnote{For sake of simplicity I consider an unweighted graph as an example}. Due to how matrices are stored the cost of keeping one in memory is $\O(n^2)$ while the cost of accessing it is just $\O(1)$.
	\item The adjacency list, which stores only a reference to the neighbour for every node, usually memorized as an array of lists, the cost of keeping such a structure in memory is $O(m + n)$ because for every node $i$ we need to store its neighbourhood $\neighbourhood{i}$.
\end{itemize}
The approach I followed for the implementation of the algorithm is, to quote the authors of~\cite{generic-he-boruvka} "a compromise between adjacency list and adjacency matrix". The \csr format is a form of encoding linearizing the structure of adjacency lists, to save space, and uses arrays to keep the graph in memory, to save time. The number of additional arrays implemented in \csr vary slightly in the literature~\cite{csr-kelly}~\cite{csr-wheatman} following the original implementation for~\cite{generic-he-boruvka} my implementation contains the following:
\begin{itemize}
	\item An \emph{outdegrees} array, for every vertex $i$ the array contains the size of its neighbourhood $|\neighbourhood{i}|$.
	\item An \emph{cumulated degrees} array, for every vertex $i$ the array contains the cumulated sum of all the neighbourhood cardinalities.
	\item An \emph{neighbours} array, which is a linearization of the adjacency list.
	\item A \emph{weights} array, containing the weight of every edge $i, j$ in the graph.
\end{itemize}
Not all of the arrays play a part in every implementation of the solver and that is because the outdegrees array was added in a later revision of the graph structure.

\bigskip
\phantomsection
\makeatletter\def\@currentlabel{\texttt{(III)}}\makeatother\label{sec:cpu-implementation}
\noindent
\textbf{CPU IMPLEMENTATION:}
\\
I will not cover at length the CPU implementation since it's quite straightforward, at first I opted
for an extremely naive implementation of Prim's algorithm, consisting of two \texttt{for} cycles.
Since the implementation worked but was in fact extremely slow if compared to finer solvers I
decided to scrap it in favour of a more efficient implementation of the Prim's algorithm based on
the use of a simple heap to sort the edges needed to build the \mst.

As I showed in~\ref{sec:intro} the time complexity of Prim's solver thus implemented is $\O(m \cdot
\log{n})$. I decided to implement Prim's algorithm instead of \brka's solver since the first one is
much simpler as far as efficiency goes the two solvers have comparable speeds.

\bigskip
\phantomsection
\makeatletter\def\@currentlabel{\texttt{(IV)}}\makeatother\label{sec:gpu-implementation}
\noindent
\textbf{GPU IMPLEMENTATION:}
\\
As I alluded to in~\ref{sec:intro} as my primary source for the development of the algorithm I chose to follow~\cite{generic-he-boruvka}. In the paper the researchers propose an higlhly efficient and parallel variant of \brka's solver for the GPU.

As will be shown later in~\ref{sec:performance-analysis} the solver runs very well on sparse graphs but struggles with more dense random graphs.

Per~\cite{generic-he-boruvka} every part of the solution can be implemented as a distinct kernel
containing a thread per vertex (such an approach is referred to as \textit{topologic} in the
literature)
\begin{enumerate}
	\item\label{item:choose-lightest} (\textit{choose lightest}) this operation picks the
		lightest connection for every vertex to the neighbourhood by simpling cycling over
		the outgoing edges, to have improved locality if two edges share the same weight pick the one with the destination id. The results are stored in the \texttt{d\_candidates} array.
	\item\label{item:mirror-removal} (\textit{Remove mirrored edges}) this operation is meant to
		remove cycles from the graph. To understand how it works let's suppose for a moment
		that the candidate edge for vertex $v$ is $(v, u)$, let's now suppose that the
		candidate edge for $u$ is the mirrored edge $(u, v)$, then the kernel will replace
		$(v, u)$ contained in \texttt{d\_candidates} with the default value
		\texttt{UINT\_MAX}.
	\item\label{item:coloration} (\textit{Initialize and propagate the colors}) this operation
		is meant to identify the connected components inside the graph and can be
		implemented as a recursive procedure having a kernel as the recursion head and then
		a series of \texttt{\_\_device\_\_} function calls to propagate the colors in the nodes.
	\item\label{item:vertex-rename} (\textit{Create new vertex ids}) to rename the vertices a
		new array is created (in my implementation is \texttt{d\_flag}) that contains a $0$
		in every position where the color of the vertex is different from the vertex id and
		a $1$ otherwise.
		
		To understand why the re-labelling is necessary we can consider for instance a graph
		in which we only have $2$ colors but one of the colored vertices is $0$ and the
		other one is $750$, if the vertex id did not change some other technique would need
		to be invented to solve the next step.
	\item\label{item:graph-contraction} (\textit{Count, assign and insert new edges}) this
		operation needs to be split in a series of simpler kernels
		\begin{itemize}
			\item\label{item:count-edges} (\textit{Count edges}) A simple kernel will be
				going through the neighbourhood of every vertex and it compares the
				color of the vertex with the color of the supervertex of the
				destination. If the colors are different it means that the two
				supervertices belong to different components.

				The result of the process will be cumulated inside an array (in my
				implementation is \texttt{d\_cumDegs})

				Afterwards a scan procedure is computed on the \texttt{d\_cumDegs}
				array, the result is going to be a fully functioning cumulated
				degree array as shown in~\ref{sec:graph-structure}.
			\item\label{item:graph-regen} (\textit{Graph regeneration}) This step
				generates the new neighbour and weight array, this is done by
				visiting every neighbour for every vertex as shown in~\ref{item:count-edges}. Edges intra-component are removed while duplicated ones are kept because it's easier than handling their removal.
		\end{itemize}
\end{enumerate}
Most of the kernels proposed in the solution can be written as \texttt{for} loops that go through
the various neighbours in the adjacency lists and, since there is no dependency between the various vertices~\cite{generic-he-boruvka}, the various steps can be computed in parallel for a major efficiency boost. Synchronization is only ever needed inside the \texttt{scan} functions, to make sure that all of the threads have a consistent view of shared memory, furthermore atomic operations are required in some instances to offer a solution to race conditions.

\begin{enumerate}
	\item The solution originally written for the CPU could not keep up with the GPU solver I therefore implemented a small Heap class that was able to move the CPU solver to the next level
	\item The GPU implementation found itself struggling essentially because of the nature of the problem and the not-so-efficient implementation
	\item Changed the GPU implementation to use an hybrid of CPU code and GPU kernels, the CPU code is used in settings such as the scan, which is more efficient than using a naive GPU kernel, as well as the contraction operation
	\item Reimplemented the contraction operation using a GPU kernel and I obtained a minor speedup. The GPU performance is still far inferior if compared with the efficient CPU version that I talked about earlier
	\item Reimplemented the scan operation to work in a work efficient way, as a source I used the preprefix file and implemented the various sequential operations in a naïve way.
	\item Reimplemented the fifth step of the algorithm, changed the logic from topological (one thread per vertex) to data centered (one thread per edge) each thread goes through a binary search of the origin of the edge in the cumDegs array.
\end{enumerate}

\bigskip
\phantomsection
\makeatletter\def\@currentlabel{\texttt{(III)}}\makeatother\label{sec:performance-analysis}
\noindent
\textbf{PERFORMANCE ANALYSIS:}
\\
Performance analysis has been carried out using an online machine through the Google Colab hosting service\footnote{\url{https://colab.research.google.com/}}, the machine specs, gathered through tools like \texttt{/proc/cpuinfo} and \texttt{/proc/meminfo}, alongside \texttt{nvidia-smi} and the official documentation~\cite{t4-info}~\cite{t4-product-brief}, are listed in~\ref{tbl:system}
\begin{center}
	\begin{longtable}{c|c|c}
		\caption{System information}\label{tbl:system}\\\hline
		\textbf{} & \textbf{CPU} & \textbf{GPU} \\
		\hline\endfirsthead\hline
		\textbf{Feature} & \textbf{CPU} & \textbf{GPU} \\
		\hline\endhead\hline\endfoot\hline\endlastfoot
		
		\textbf{Manufacturer} & Intel & NVIDIA \\\hline

		\textbf{Model} & Intel Xeon (V\footnote{V stands for virtualized, can't find the actual CPU model that is being mounted in servers}) & T4 \\\hline
		\textbf{Launch Date} & N.A. & Oct 2018 \\\hline
		\textbf{\#Cores} & $2$ vcores  & $2560$ C\footnote{CUDA cores}, $320$ T\footnote{Tensor cores} \\\hline
		\textbf{Clock Speed (GHz)} & $2.20$ GHz & Base: $0.585$ MHz, Boost: $1.59$ GHz \\\hline
		\textbf{Memory / Cache} & L3 Cache: $56$ MB & VRAM: $16$ GB GDDR6 \\\hline
		\textbf{Memory Bandwidth} & N.A. & 300 GB/s \\\hline
		\textbf{TDP (Thermal Design Power)} & N.A. & $70$ W \\\hline
		\textbf{Architecture} & N.A. & Turing \\\hline
	\end{longtable}

\end{center}

\bigskip
\phantomsection
\makeatletter\def\@currentlabel{\texttt{(IV)}}\makeatother\label{sec:final-thoughts}
\noindent
\textbf{FINAL THOUGHTS:}
\\

\clearpage

\printbibliography

\end{document}
